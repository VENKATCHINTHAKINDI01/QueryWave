llm:
  provider: local
  model_name: default
  temperature: 0.2
  max_tokens: 1024
